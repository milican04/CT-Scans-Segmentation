{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pleased-thanksgiving",
   "metadata": {},
   "source": [
    "# Segmentation of Lung CT scans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-prospect",
   "metadata": {},
   "source": [
    "Data used in project can be downloaded via following link: http://medicaldecathlon.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-american",
   "metadata": {},
   "source": [
    "## Intorduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-arrangement",
   "metadata": {},
   "source": [
    "The idea of this project is to take input data in the form of a CT scan of the lungs, create and train a model which will do their segmentation and find abnormalities if they exist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-synthesis",
   "metadata": {},
   "source": [
    "The typical use of convolutional neural networks is on classification tasks, where the output of an image is a single class label. In the problem that we are considering, we definitely should classify CT scans, but we also need to locate abnormalities if they exist. To do that, it is necessary to assign a label to each pixel of the CT scan. Of course, we are going to have two labels: normal and abnormal. All pixels that are classified as abnormal, together construct the abnormality of the considering lungs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-sailing",
   "metadata": {},
   "source": [
    "The model will be a convolutional neural network based on the U-net architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-pledge",
   "metadata": {},
   "source": [
    "#### What is U-net architecture and why we are going to use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-fence",
   "metadata": {},
   "source": [
    "The U-net architecture, the so-called \"fully convolutional network,\" was designed in 2015. for the segmentation of biomedical images. U-Net has been successfully used in numerous studies and clinical applications for CT scan segmentation. \n",
    "\n",
    "It has a symetric structure in the shape of the letter U, and it consists of two main parts: the contracting path (coder) and the expansive path (decoder). This symmetric design of U-Net ensures a balanced extraction and reconstruction of features, leading to more accurate segmentation, which will be very useful considering the complex lung structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b49b6c-c744-43f2-a112-0eefd782c615",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b5ed27d-523c-478c-9bf0-7c42f1d75df4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import resize\n",
    "from torchvision.io import read_image\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bd7f4b7-9c85-4282-9006-796e3c2f51ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.66.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-helmet",
   "metadata": {},
   "source": [
    "## Data Load and Preproccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b20b75c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ct_to_slices(path):\n",
    "    data = nib.load(path).get_fdata()\n",
    "    [_, _, slices] = data.shape\n",
    "    \n",
    "    return [data[..., slice] for slice in range(slices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38cf9bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ct_dataset_to_slices(dataset_dir, output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    \n",
    "    images_dir = os.path.join(output_dir, 'imagesTr')\n",
    "    labels_dir = os.path.join(output_dir, 'labelsTr')\n",
    "    os.mkdir(images_dir)\n",
    "    os.mkdir(labels_dir)\n",
    "    \n",
    "    image_paths = None\n",
    "    label_paths = None\n",
    "    \n",
    "    with open(os.path.join(dataset_dir, 'dataset.json'), 'r') as dataset_info:\n",
    "        data = json.load(dataset_info)\n",
    "        image_paths = [os.path.join(dataset_dir, scan['image']) for scan in data['training']]\n",
    "        label_paths = [os.path.join(dataset_dir, scan['label']) for scan in data['training']]\n",
    "        \n",
    "    for (i, path) in enumerate(image_paths):\n",
    "        for (j, slice) in enumerate(ct_to_slices(path)):\n",
    "            plt.imsave(os.path.join(images_dir, f'{i + 1}_{j + 1}.png'), slice, cmap='gray')\n",
    "    \n",
    "    for (i, path) in enumerate(label_paths):\n",
    "        for (j, slice) in enumerate(ct_to_slices(path)):\n",
    "            plt.imsave(os.path.join(labels_dir, f'{i + 1}_{j + 1}.png'), slice, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace483db-ae8d-4efd-97d0-d37aee7e2821",
   "metadata": {},
   "source": [
    "### Clicing 3D scans into 2D images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af68bc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_ct_dataset_to_slices('Task06_Lung', 'Task06_Lung_Sliced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf933d1-51f9-47b7-96b3-cdf2f78087d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load first image\n",
    "image = Image.open('Task06_Lung_Sliced/imagesTr/1_1.png')\n",
    "# image.show()\n",
    "\n",
    "print(f\"Image size (width, height): {image.size}\")\n",
    "num_channels = len(image.mode)\n",
    "print(f\"Number of channels: {num_channels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73c1689-f80e-446b-abbe-3f48a43509f9",
   "metadata": {},
   "source": [
    "Before we load the data into the training set, we will need to downsize the images as previous size is too large!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6e5f6674-4f2e-4064-a2ea-e1aaf0a0dbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 128\n",
    "IMG_HEIGHT = 128\n",
    "IMG_CHANNELS = num_channels\n",
    "\n",
    "SLICED_DATA = 'Task06_Lung_Sliced'\n",
    "\n",
    "IMAGES_TR_PATH = SLICED_DATA + \"/imagesTr\"\n",
    "LABELS_TR_PATH = SLICED_DATA + \"/labelsTr\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33f97d2-0fee-4e8d-a5f6-ca240ddc9f98",
   "metadata": {},
   "source": [
    "Function to load sliced images, resize them and store them into np matrix, which we will use data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1b4e71e2-2f83-467c-839a-b05ec72766eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadAndResize(images_names, dirPath, channels, height, width):\n",
    "    storage = np.zeros((len(images_names), channels, height, width), dtype=np.uint8)\n",
    "    for n, id_ in tqdm(enumerate(images_names), total=len(images_names)):   \n",
    "        # load image and resize it\n",
    "        img = read_image(dirPath + \"/\" + id_).float()  # [C, H, W] format\n",
    "        img = resize(img, [height, width])  # Resize the image\n",
    "        \n",
    "        # normalize pixels\n",
    "        img /= 255.0\n",
    "\n",
    "        # store resized image\n",
    "        storage[n] = img\n",
    "    return storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccad10a-7eec-404a-a6f0-4bf573ee1f16",
   "metadata": {},
   "source": [
    "### Storing data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "771cead6-16b7-4724-bc88-d928338e12b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = next(os.walk(IMAGES_TR_PATH))[2]\n",
    "# print(train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "82ebb798-1f29-461e-ba89-f9c8291ba4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/17657 [00:00<?, ?it/s]/tmp/ipykernel_6480/2804555441.py:12: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  storage[n] = img\n",
      "100%|█████████████████████████████████████| 17657/17657 [05:04<00:00, 58.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1]]], dtype=uint8)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = loadAndResize(train_ids, IMAGES_TR_PATH, IMG_CHANNELS, IMG_HEIGHT, IMG_WIDTH)\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6c61f2-8eee-43b0-827f-86471d0d24c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/17657 [00:00<?, ?it/s]/tmp/ipykernel_6480/2804555441.py:12: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  storage[n] = img\n",
      " 83%|█████████████████████████████▉      | 14659/17657 [01:05<00:12, 239.66it/s]"
     ]
    }
   ],
   "source": [
    "Y_train = loadAndResize(train_ids, LABELS_TR_PATH, IMG_CHANNELS, IMG_HEIGHT, IMG_WIDTH)\n",
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-first",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa200f2-d090-4dd2-a821-edb70fe78fc1",
   "metadata": {},
   "source": [
    "### U-Net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20d1bcfa-5a7d-45fa-a0f7-b3de3ee1f205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mislila sam da kada skontamo koje cemo dimenzije filtera itd da napravim ovakvu sliku za nasu arhitekturu i ubcaim ovako ako se slazes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe42eb9-2c83-4862-ba0e-39fb3ae15134",
   "metadata": {},
   "source": [
    "<img src=\"metadata/u-net-architecture.png\" alt=\"Alt Text\" width=\"650\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebd7a1ef-0c6b-4535-969e-c2c204413135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ovde sam rendom stavila neke brojke koje sam nasla na netu, podesicemo posle tacno sta zelimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dec0bb3-3561-40da-973f-622b70c899d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# razlika izmedju tensorflow-a i torcha je sto ovde imamo ovu forward fju kojom definisemo \"kretanje\" izmedju slojeva, \n",
    "# nije sve u jednoj liniji kao za tensorflow\n",
    "# mozda ima i drugi nacin, ja sam nasla ovaj, ako ti se ne svidja mozemo da promenimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8776cb9e-d4db-4d91-a7d0-d963a8a9fbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: opciona funkcija za crtanje izmedju slojeva"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1b1d38-767f-4b10-9fed-1a51cad8179d",
   "metadata": {},
   "source": [
    "Creating U-Net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "removable-brunswick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model class\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Coder\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv1_drop = nn.Dropout(0.1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv2_drop = nn.Dropout(0.1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3_drop = nn.Dropout(0.2)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv4_drop = nn.Dropout(0.2)\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv5_drop = nn.Dropout(0.3)\n",
    "        \n",
    "        # Max pooling layers\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Decoder\n",
    "        self.upconv1 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.upconv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.upconv3 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.upconv4 = nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(16, 1, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Coder\n",
    "        c1 = F.relu(self.conv1(x))\n",
    "        c1 = F.relu(self.conv1_drop(self.conv1(c1)))\n",
    "        p1 = self.pool(c1)\n",
    "        \n",
    "        c2 = F.relu(self.conv2(p1))\n",
    "        c2 = F.relu(self.conv2_drop(self.conv2(c2)))\n",
    "        p2 = self.pool(c2)\n",
    "        \n",
    "        c3 = F.relu(self.conv3(p2))\n",
    "        c3 = F.relu(self.conv3_drop(self.conv3(c3)))\n",
    "        p3 = self.pool(c3)\n",
    "        \n",
    "        c4 = F.relu(self.conv4(p3))\n",
    "        c4 = F.relu(self.conv4_drop(self.conv4(c4)))\n",
    "        p4 = self.pool(c4)\n",
    "        \n",
    "        c5 = F.relu(self.conv5(p4))\n",
    "        c5 = F.relu(self.conv5_drop(self.conv5(c5)))\n",
    "        \n",
    "        # Decoder\n",
    "        u6 = self.upconv1(c5)\n",
    "        u6 = torch.cat([u6, c4], dim=1)\n",
    "        c6 = F.relu(self.conv4(u6))\n",
    "        c6 = F.relu(self.conv4_drop(self.conv4(c6)))\n",
    "\n",
    "        u7 = self.upconv2(c6)\n",
    "        u7 = torch.cat([u7, c3], dim=1)\n",
    "        c7 = F.relu(self.conv3(u7))\n",
    "        c7 = F.relu(self.conv3_drop(self.conv3(c7)))\n",
    "\n",
    "        u8 = self.upconv3(c7)\n",
    "        u8 = torch.cat([u8, c2], dim=1)\n",
    "        c8 = F.relu(self.conv2(u8))\n",
    "        c8 = F.relu(self.conv2_drop(self.conv2(c8)))\n",
    "\n",
    "        u9 = self.upconv4(c8)\n",
    "        u9 = torch.cat([u9, c1], dim=1)\n",
    "        c9 = F.relu(self.conv1(u9))\n",
    "        c9 = F.relu(self.conv1_drop(self.conv1(c9)))\n",
    "\n",
    "        output = torch.sigmoid(self.final_conv(c9))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a7bc4e3-28ff-4010-b5d4-9bbde83ca191",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv1_drop): Dropout(p=0.1, inplace=False)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2_drop): Dropout(p=0.1, inplace=False)\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_drop): Dropout(p=0.2, inplace=False)\n",
      "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_drop): Dropout(p=0.2, inplace=False)\n",
      "  (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_drop): Dropout(p=0.3, inplace=False)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (upconv1): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (upconv2): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (upconv3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (upconv4): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (final_conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Mmdel instantiation\n",
    "model = UNet()\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Print model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681b28b3-7559-4b94-86b9-463256f9fe3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd12910-b46a-45b8-a67c-e5e1e8694137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "noticed-branch",
   "metadata": {},
   "source": [
    "## Model evaluation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-sleep",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-consultation",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-moral",
   "metadata": {},
   "source": [
    "1. U-Net: Convolutional Networks for Biomedical Image Segmentation: Olaf Ronneberger, Philipp Fischer, and Thomas Brox - Computer Science Department and BIOSS Centre for Biological Signalling Studies, University of Freiburg, Germany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-shape",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
