{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b558d79-713b-4293-b5f8-fb1589c1a368",
   "metadata": {},
   "source": [
    "# CT scan segmetation using multiple slices per image - Liver dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7808ab67-ef95-434b-90aa-419e7f183a6b",
   "metadata": {},
   "source": [
    "As we are using 2D images to describe 3D CT scans, inevitably we are losing some information about original CT scan.\n",
    "In this part we will try to give our model more information and still keep using 2D aproach. \n",
    "In previous aproach, we were slicing 3D scans and providing each of those slices to the model to learn from them but separately. So, our model doesn't know that slices are actually connected and represent a part of some bigger media.\n",
    "Now, we will merge each slice with its previous and next slice of CT scan, in order to provide more information to out model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81bb3e8f-9adf-4363-ad23-6e7dcf03669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../Data-Preprocessing.ipynb\n",
    "%run ../U-Net.ipynb\n",
    "%run ../Train-Eval-Utils.ipynb\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader, ConcatDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e5cc13-ee1a-48c5-93d1-3a0323723cf8",
   "metadata": {},
   "source": [
    "Data will be preprocess on the same way as before. So, we still slicing the 3D CT scan into 2D images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc8b7c25-3364-4e69-8501-fb1d6de5a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run if this is the first run of liver segmentation:\n",
    "# convert_ct_dataset_to_slices('Task03_Liver', 'Liver_Train', 'Liver_Val', 'Liver_Test', val_split=0.1, test_split=0.1, negative_downsampling_rate=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54a0eb1-b1e0-40f2-9040-484ccb47fc8f",
   "metadata": {},
   "source": [
    "In the previous approach, the model received one slice (the one on which we want to do the segmentation), and now, in addition to the current slice, we pass the previous and the next slice of the CT scan to the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b693b148-7c9c-4211-bcd1-c5c4cb904075",
   "metadata": {},
   "source": [
    "As the slices are black and white, we can merge them into one 3-channel image and pass it to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414cf9ee-bb7e-4df3-8a25-6a622c3852d7",
   "metadata": {},
   "source": [
    " <img src=\"../metadata/multipleSlicesForTrainingLiver.png\" alt=\"multiple slices image\" width=\"500\" height=\"600\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4954a00f-eea5-42a1-9057-aaf6390bdeb9",
   "metadata": {},
   "source": [
    "According to that, we created CTDatasetMultiSlices class which will prepare our data on the we way descrribed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5768bd1-00f7-4baf-9b22-c139fe512d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128), antialias=False),\n",
    "    transforms.ConvertImageDtype(torch.float)\n",
    "])\n",
    "\n",
    "TRAIN_DIR = 'Liver_Train'\n",
    "\n",
    "train_dataset = CTDatasetMultiSlices(root_dir=TRAIN_DIR, image_transform=transform, label_transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=os.cpu_count())\n",
    "\n",
    "VAL_DIR = 'Liver_Val'\n",
    "\n",
    "val_dataset = CTDatasetMultiSlices(root_dir=VAL_DIR, image_transform=transform, label_transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True, num_workers=os.cpu_count())\n",
    "\n",
    "TEST_DIR = 'Liver_Test'\n",
    "\n",
    "test_dataset = CTDatasetMultiSlices(root_dir=TEST_DIR, image_transform=transform, label_transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea33a57-f2a7-46c2-9ef7-6edde76cd1ae",
   "metadata": {},
   "source": [
    "If we look more closely at any of the training instances, we notice that they are blurry. This is due to the fact that there are actually three images in one instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5806107-0450-4c04-a90d-5364747b02ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class demo\n",
    "image, label = train_dataset.__getitem__(75)\n",
    "\n",
    "plt.imshow(image.permute(1, 2, 0).numpy())  # No cmap for 3-channel images\n",
    "plt.axis('off')  # Turn off axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc2115c-c6a9-4531-801c-d2f31c2c8d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f6d5db-111b-4ea3-a21b-4e766d37f0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = DiceLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ae10d0-b91f-4c84-a610-d1537d5d25a3",
   "metadata": {},
   "source": [
    "Further, we will train the model with different parameters of u-net network that we created and, based on the validation set, conclude which model we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6230aca4-0b37-4bb7-984d-bf19fb95bd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = UNet(depth=3, in_channel=3)\n",
    "model_3.to(device)\n",
    "optimizer = torch.optim.Adam(model_3.parameters(), lr=0.001)\n",
    "_, best_loss = train_loop_with_validation(model_3, 30, train_loader, val_loader, optimizer, criterion)\n",
    "print(f'Best loss achieved on the validation set: {best_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56db3b40-b817-4a0d-8c89-44e97837329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = UNet(depth=5, in_channel=3)\n",
    "model_4.to(device)\n",
    "optimizer = torch.optim.Adam(model_4.parameters(), lr=0.001)\n",
    "_, best_loss = train_loop_with_validation(model_4, 30, train_loader, val_loader, optimizer, criterion)\n",
    "print(f'Best loss achieved on the validation set: {best_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cc3f9e-ca76-412b-80ad-dc8b668a6b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5 = UNet(depth=7, in_channel=3)\n",
    "model_5.to(device)\n",
    "optimizer = torch.optim.Adam(model_5.parameters(), lr=0.001)\n",
    "_, best_loss = train_loop_with_validation(model_5, 30, train_loader, val_loader, optimizer, criterion)\n",
    "print(f'Best loss achieved on the validation set: {best_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f75a77-24ec-42a1-8815-562768862bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lower_lr = UNet(depth=3, in_channel=3)\n",
    "model_lower_lr.to(device)\n",
    "optimizer = torch.optim.Adam(model_lower_lr.parameters(), lr=0.0005)\n",
    "_, best_loss = train_loop_with_validation(model_lower_lr, 60, train_loader, val_loader, optimizer, criterion)\n",
    "print(f'Best loss achieved on the validation set: {best_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67f11a8-2e5a-4501-808a-4f09049ac7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_concat_dataset = ConcatDataset([train_dataset, val_dataset])\n",
    "train_val_concat_loader = DataLoader(train_val_concat_dataset, batch_size=32, shuffle=True, num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e155bc27-510b-4394-83f6-ecd9ee4bf909",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final = UNet(in_channel=3, depth=3)\n",
    "model_final.to(device)\n",
    "optimizer = torch.optim.Adam(model_final.parameters(), lr=0.001)\n",
    "best_model, best_loss = train_loop(model_final, 50, train_val_concat_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89386279-6514-4a59-9cf2-218e33050ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model.state_dict(), '../models/modelLiverMultiSlices.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6056e3-add8-4136-a1e6-ddd50ef77634",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_predictions(best_model, train_loader, device, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53c8e60-6462-499b-bd81-008c9a084ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_predictions(best_model, test_loader, device, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ae4826-eb09-41c9-a075-8a2fcc792122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
